{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rP-UrzBZ282_",
        "r1m2DhGD3o8z",
        "EjTFLcmm3L2p",
        "n4WrZkbondDR"
      ],
      "authorship_tag": "ABX9TyPnYPOaDvV+BbBIhVRKOliA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anytaaly/NLP-Advanced/blob/master/NLP_ADVANCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notebook Syntax and Explaination"
      ],
      "metadata": {
        "id": "CeadivKk0AZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! jq  \"del(.metadata.widgets)\" \"/content/drive/MyDrive/NLP-ADVANCE.ipynb\""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix7QDU7Ynln9",
        "outputId": "b2bfddcf-b028-4f4c-9792-8a961cdec4f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: jq: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Utc0jHWInbqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#%time"
      ],
      "metadata": {
        "id": "lTUBz5sS3Ap_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time"
      ],
      "metadata": {
        "id": "2KUAh6HO0iAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Jupyter notebooks, syntax that starts with `%` is called a magic command. Magic commands are special commands provided by IPython (which Jupyter is built on top of) to make interactive work easier.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CdFxfa6I0jsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Magic Command | What it does  |\n",
        "|---|---|\n",
        "| %time | Times how long a single run of a line takes|\n",
        "| %timeit | Runs the line multiple times and reports averahe time |\n",
        "| %env | Sets environment variables |\n",
        "| %matplotlib inline | Displays matplotlib plots inline in the notebook |\n",
        "| %run script.py | Runs a Python script inside your notebook|"
      ],
      "metadata": {
        "id": "4LLNhsEy1bxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "id": "fuLwzM0E0ADf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Explainnation of the command:\n",
        "`CUBLAS_WORKSPACE_CONFIG=:4096:8` is typically used in Jupyter notebooks to set an environment variable CUBLAS_WORKSPACE_CONFIG before running GPU-intensive code (usually PyTorch or TensorFlow). This variable configures how cuBLAS (NVIDIA’s CUDA Basic Linear Algebra Subprograms library) uses workspace memory on the GPU."
      ],
      "metadata": {
        "id": "8YGwDRuj0WTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#%capture"
      ],
      "metadata": {
        "id": "rP-UrzBZ282_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture captured_output\n",
        "print(\"Hello from the hidden world!\")\n"
      ],
      "metadata": {
        "id": "JNWTo-to2bGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Later...\n",
        "print(captured_output.stdout)"
      ],
      "metadata": {
        "id": "zWl4v-e620SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%%capture `is a cell magic that lets you suppress output from a code cell — perfect for hiding messy logs, unwanted print statements, or warnings when you just want to run something quietly.\n",
        "\n"
      ],
      "metadata": {
        "id": "hjLH8CrT21nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#%%writefile"
      ],
      "metadata": {
        "id": "r1m2DhGD3o8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "CUBLAS_WORKSPACE_CONFIG: \":4096:8\"\n",
        "use_deterministic_algorithms: true"
      ],
      "metadata": {
        "id": "QBRm9Hfc3rjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create (or overwrite) a file named ` notebook-config.yaml` with that content."
      ],
      "metadata": {
        "id": "Gg_r0WpT3wb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where is the file saved?\n",
        "Right in your current working directory (the same place as where you run `!ls` or `!pwd` from a cell)."
      ],
      "metadata": {
        "id": "-TlFWNaB30hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To append instead of overwrite, use:\n",
        "\n",
        "\n",
        "```\n",
        "# %%writefile -a filename\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "j2pkbeDe373F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shell Terminal Commands in Jupyter"
      ],
      "metadata": {
        "id": "IQsCBZV83EFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#!your-shell-command"
      ],
      "metadata": {
        "id": "EjTFLcmm3L2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "LsIWmZxw3Sk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo"
      ],
      "metadata": {
        "id": "fcKkLf0B3WqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"text.txt\"\n",
        "!echo \"this is a test\" > {filename}\n",
        "!cat {filename}"
      ],
      "metadata": {
        "id": "NOY6zcoq3YsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VaoF0c-o3WhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 1: Introduction\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "n4WrZkbondDR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yc7140ZnnfHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 2: Tokenization\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "HJLvxjvLnhCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 3: Basic Preprocessing\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "WVXoFBmInnG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W8xI5427nrRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 4: Advanced Preprocessing\n",
        "### *Course Source: NLP Demystified*\n"
      ],
      "metadata": {
        "id": "_fVeDB2pnudz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 5: Basic Bag-of-words\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "th_N7tH3n4MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 6: TF-IDF\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "jFmgcOFYn-ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 7: Building Models\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "dj-NhhHkoDDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 8: Naive Bayes\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "mPnuITiioIsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 9: Topic Modelling\n",
        "### *Course Source: NLP Demystified*\n"
      ],
      "metadata": {
        "id": "WFRoKgytoMJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter10: Text Classification & Sentiment Analysis\n",
        "\n",
        "### *Course Source: UPENN CIS 5300*\n"
      ],
      "metadata": {
        "id": "4N2DF7emmWVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This week we'll dive into the problem of text classification using the example task of sentiment analysis, which is the process of determining the emotion that an author expresses towards a subject in their writing Like in restaurant reviews on Yelp or product reviews on Amazon. Or movie reviews on Rotten Tomatoes.\n",
        "\n",
        "Sentiment analysis is a technique used to identify and categorize subjective information in text, such as opinions, emotions and attitudes.\n",
        "\n",
        "a baseline approach to sentiment analysis and learn how to classify text using the naive Bayes algorithm.\n",
        "\n",
        "This Chapter also introduce important concepts like parameter estimation and evaluation metrics."
      ],
      "metadata": {
        "id": "SdFjU5ZDk5bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification lies at the heart of both human and machine intelligence. Deciding\n",
        "what letter, word, or image has been presented to our senses, recognizing faces\n",
        "or voices, sorting mail, assigning grades to homeworks; these are all examples of\n",
        "assigning a category to an input.\n",
        "\n",
        " An example of sentiment analysis is to categorize movie reviews as either positive or negative.Within the movie review, we'll often see phrases that are indicative of positive or negative sentiments towards the movie.\n",
        "\n",
        " | Category | Review Text |\n",
        "|---|---|\n",
        "| - | Unbelievably disappointing|\n",
        "| + | Full of zany characters and richly applied satire, and some great plot twists |\n",
        "| + | This is the great screwball comedy ever filmed. It was pathetic. The worst part of about it was the boxing scenes. |\n",
        "| - |...awful pizza and ridiculously overpriced...|\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1fwIAci_wRAfrqZ4QNS6NscrrUKSngLXd)\n",
        "\n",
        "\n",
        "So, movie reviews is one example of text classification where the classification is a simple binary task.\n",
        "\n",
        "\n",
        "\n",
        "Below image has 2 distint reviews from Pag and Lee database. In the 2nd review, you will see positive words and the last line depicts that it indeed is a nagtive review.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Rd2IWD8eVg83WgVdyntLLIw2O6ej4XTw)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ar7IIS3lmG1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Other example fo the classification are:\n",
        "\n",
        "\n",
        "*   Assigning subject categories, topics, or genres\n",
        "*  Spam Detection\n",
        "*  Language Detection\n",
        "* Another example is to identify/predict the gender of the Author.  (Authorship Attribute identification )\n",
        "*   Or determine the social economic background of the user.\n",
        "*  Another example of text classification is taxonomy of the medical document.\n"
      ],
      "metadata": {
        "id": "ZDs_MIZ4paiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment analysis:"
      ],
      "metadata": {
        "id": "SAQm6pBd8hp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We focus on one common text categorization task, sentiment analysis, the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object.\n",
        "\n",
        "\n",
        "The simplest version of sentiment analysis is a binary classification task, and the words of the review provide excellent cues.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Gu5pIGCUmizC0c4o4Uc2hiAdmBWom5Hc)\n",
        "\n"
      ],
      "metadata": {
        "id": "w05kMo7grOFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Approach for Sentiment Analysis: Naives Bayes\n",
        "Naives Bayes is a probabilistic classifier, meaning that for a document d, out pf all classes c $ \\in $ C the classifier retuns the class $c^$ which has the maximum posterior probability given the document.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A Probability is just a number between 0 and 1 representing how likely something will happen or how likely a proposition is True. Sometimes this number is repeated observation over time or it could just represnt a belief in something.\n",
        "\n",
        "If you flip a coint twice, the probability of the 2nd toss is independent on the first tos.. thats an independent Events.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1z9RSldzjUSoGRm19-mzCPUr40zMGsxOU)\n",
        "\n",
        "P(HH) = P(H) X P(H) = 1/4\n",
        "\n",
        "\n",
        "Conditional Probability:\n",
        "P(A and B) = P(A) x P(B | A) x P(B) = 1/4\n",
        "above reads, probablity of getting Head after getting head in the first Toss. so basically Head and Head.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1BCAXTnlEUPgiVnblpioCTp94CSUikht0)\n",
        "\n",
        "Instead of A and B, lets replace it with H and D.\n",
        "Where H is some Hypothesis that we hold and D is the some body of data and togehter this is the probability of our hypothesis or belief being true\n",
        "given some observe data.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1C2MTCavsV0Dni6hm1fEK5B6Lf6hKUW)\n",
        "\n",
        "These theorem provides a way to update how strongly we hold a belief as new data becomes available.\n",
        "\n",
        "For example, you may be running some subscription service and have some initial belief about whether a customer will sign up for it, and as new data becomes available through the customer taking certain actions that belief adjust. Perhaps the customer signs up for a trial subscription or they respond to a survey emails or contact customer service, all of which can affect the strength of your belief overtime.\n",
        "\n",
        "This probablity that we are trying to calculate is called the posterior probability.\n",
        "\n",
        "Let's look at the other parts of the theorem and then walk through a spatial example.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1C2MTCavsV0Dni6hm1fEK5B6Lf6hKUWje)\n",
        "\n",
        "\n",
        "To calculate the posterior probability, we need to quantify the strength of our initial belief before any evidence or additional data taken into account. This is called prior. In addition we need the probability of observing our data given our beliefs. This is the likelihood, multiplying the prior and likelihood gives us the posterior. We often then divide by the probability of the data to normalize the probability to be between zero and one this is often referred to as the normalizing constant.\n",
        "\n",
        "The prior and likelihood aren’t always easy to get. Bu in our case fortunately, we will be able to calculate all with our datasets. In other cases sometimes intuition or rough estimation will be needed.\n",
        "\n",
        "\n",
        "Let’s work through a text classification.\n",
        "\n",
        "Let’s see a corpus of 90 movie reviews. Each square represents a single review. These reviews are labeled to either positive or negative. using bayes theorem. We want to find out the probability of a movie review being positive given it contains the word brilliant.\n",
        "First we calculate the prior. There are 90 movie reviews in total 49 of which are positive so that’s 49/90.\n",
        "Next we calculate the likelihood that is the probability of the word brilliant occurring given a positive review. Let's say we go through and we find the movie reviews containing the word, brilliant and it looks like this:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1FAll7MtvmrkCY1j1KWoWHmGhm0bN0vXy)\n",
        "\n",
        "\n",
        " The word is present in both positive and negative reviews. Calculate the likelihood then we need to look at the positive reviews and among those how many contain the word brilliant and divided by the total number of positive reviews which gives us 14/49 Probability.\n",
        "\n",
        "Finally our normalizing constant is the probability of a movie review containing the word \"brilliant\"\n",
        "\n",
        "P(Positive) = 49/90\n",
        "P(\"Brilliant\" | Positive) = 14/49\n",
        "P(\"Brilliant\") = 20/90  \n",
        "\n",
        "(20 reviews have word \"Brilliant\" in it and 14 of those are rositive).\n",
        "if we work it out all according to the formula. The probability of a review being positive given a word \"brilliant is 0.7\n",
        "\n",
        "$\n",
        "P(positive | \"brilliant\") = \\frac{14}{49} * \\frac{49/90}{20/90}\n",
        "$\n",
        "\n",
        "The point here is that if you are in a situation where you have some data regarding prior and likelihood then you can calculate the probability of your hypothesis given some data.\n",
        "You can also calculate the likelihood if you have the posterior.\n",
        "\n",
        "\n",
        "Bayes' Theorem:\n",
        "$$\n",
        "argmax_{c∈C}  P(c|d) = argmax_{c∈C}  \\frac{P(c) P(d|c)}{P(d)}\n",
        "$$\n",
        "\n",
        "Probability of Certain class given a document. specifically we're more interested in which class is the most probable.\n",
        "Specifically, find the class which maximizes the posterior possibility ( i.e. the most probable class).\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1QJCeYjdViLllYtcYZbtxvICl5apfdUN0)\n",
        "\n",
        "in addition, because the document is static in our calculation, we do not need to normalize it.\n",
        "\n",
        "Calculating the prior is straightforward, the probability of a certain class occuring is just the number of documnet  of that class divided by the toal number os documents.\n",
        "![](https://drive.google.com/uc?export=view&id=1lDizUXiXrOzhYdL9x5tlOidVXwWOwhyK)\n",
        "\n",
        "and the likilihood is the probability of particular sequence of words given a class.\n",
        "\n",
        "to get the probablity of sequence of words, we would need alot of data and thats where Naive Bayes theorem plays a role. Instead of the sequence and order we look at the probability of each word given a particular class.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1MR0F7BcCgRFHq2h9mYDICVEBSq94D_3X)\n",
        "\n",
        "\n",
        "there is one issue, we will be multiplying a lot of small probaibilities here. and if a document is long enough with a diverse vocabulary, this could lead to underflow where the number is so that computer cant represent in memory. so we use log instead.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1GsgSc8mr8f2926FnQ3wgAh7girl4UO5Q)\n"
      ],
      "metadata": {
        "id": "W8BIEEcouVcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "| Document  | Class |\n",
        "|---|---|\n",
        "| Beyond the pandemic| Medical|\n",
        "| Compelling jumble of philosopgy | Not medical |\n",
        "| Vaccine rollout enters second phase | Medical |\n",
        "| Man regains sight with implant | Medical |\n",
        "| My toilet is haunted | Not Medical |\n",
        "\n",
        "\n",
        "3 Medical documents, 3 Medical documents.\n",
        "all w = 21\n",
        "Medical Vocab = 13 {Beyond, the pandemic, Vaccine, rollout, enters, second, phase, Man, regauins, sight , with , implant}\n",
        "\n",
        "Priors, P(c) can calcuated as:\n",
        "\n",
        "Prior (medical) = 3/5\n",
        "Priot (not medical) = 2/5\n",
        "\n",
        "\n",
        "\n",
        "Likelihood Calculation can be calculated by each word given by each class that is\n",
        "Count of w(i) in c / total number / count of all w in c\n",
        "\n",
        "$$\n",
        " P(vaccine|medical) =   \\frac{1}{13}\n",
        "$$\n",
        "$$\n",
        " P(haunted | medical) = \\frac{0}{13}\n",
        "$$\n",
        "\n",
        "we add a smoothing parameter to avoid words thta doesnt occur in our corpus. alpha (any arbitrary number) in numberator and alpha|v| to the denominator.\n",
        "\n",
        "$$\n",
        " P(vaccine|medical) =   \\frac{1 + 1 }{13 + 21} =\n",
        "$$\n",
        "\n",
        "Our classifier is now trained.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1yPZfeYXFegIitOEtwxU-uNCY8ahoyVTL)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1vLGwCJKdCm6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuwliXTzbaNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s work through a text classification."
      ],
      "metadata": {
        "id": "kzrNPsccRT8d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfujRHNVscDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycozQkS0pSIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4vHWadf3DHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10 Regular Expressions in NLP"
      ],
      "metadata": {
        "id": "yFcYeNYDlNyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 12: Text Processing & Logistic Regression\n",
        "### *Course Source: UPENN CIS 5300*"
      ],
      "metadata": {
        "id": "iVXSyepJmfZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ORFh22yFn9is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 13: Review of Probabilities & N-gram Langauge Models\n",
        "`### *Course Source: UPENN CIS 5300*`\n"
      ],
      "metadata": {
        "id": "ZA5lubIJmr6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iCP_RkrznE3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 14:  N-gram Langauge Models"
      ],
      "metadata": {
        "id": "g8mI9gE0nGJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram and perceptrons are fundamentally different approaches to natural language procesisng tasks:\n",
        "\n",
        "##Ngrams\n",
        "Ngrams are simple statistical language models that predict the next word based on the previous n-1 words.\n",
        "\n",
        "*   **What they are**: Sequences of n adjacent words or characters from a text.  \n",
        "*   **How they work**: Count frequency of word sequences in training dara.\n",
        "*   **Example** In a bigram (n=2) model trained on \"The cat sat on the mat\": \"The cat\" -> might predict \"sat \" as likely next word.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9xlE38fEnbM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 15: Vector Space Semantics\n",
        "`Course Source: UPENN CIS 5300`\n"
      ],
      "metadata": {
        "id": "5dWPPmrKm0Em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 16: Vector Space Models\n",
        "### *Course Source: UPENN CIS 5300*"
      ],
      "metadata": {
        "id": "X-X8EvUNm_qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 17: Neural Language Models\n",
        "### *Course Source: UPENN CIS 5300*"
      ],
      "metadata": {
        "id": "W6fg4hQKnEnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the Ngram based language models, Neural language models use word embedding based representations for thier contexts.\n",
        "\n",
        "\n",
        "This allows them to make much better probabilistic prediction about the next word in the sequence, and they've become the foundation for large pre-trained language models like ChatGPT, that have led to exciting innovations in the field of NLP.\n",
        "We'll start with an introduction to neural networks. We'll discuss the analogy to the human brain and then go into the mathematical notation that we use for neural networks. We'll then discuss how the representational power of neural networks comes from combining many neural units, which are similar to the logistic regression units that we studied earlier, and from combining them through hidden layers that are stacked and contain many neural units.\n",
        "We'll then cover some of the details of how we go about training the parameters of neural networks. Next, I'll present one of the earliest neural probabilistic language models, which was published in a NeurIPS paper by Yoshua Bengio in 2003. We'll explore how this model used word embeddings as input, and how the word embeddings themselves were a byproduct of the training procedure for the neural probabilistic language model.\n",
        "We'll also discuss the advantages and disadvantages of using this kind of neural net-based language model compared to the traditional approach of using ngram based language models. Finally, we'll take a look at other neural network-based architectures for language models, and we'll give you a broad understanding of the different approaches that have been used to build language models using neural networks.\n",
        "\n",
        "\n",
        "\n",
        "| Feature   | N-Gram Model  | Neural language Model  |\n",
        "|---|---|---|\n",
        "| Representation | Use raw characters or words | Uses word embeddings |\n",
        "| Context Size  | Fixed (n characters/words) | Dynamic, can consider entire sentence  |\n",
        "| Handling Unknown Words  |Zero probability for unseen words |  Generaliozes using embeddings  |\n",
        "| Probability Calculation  |Frequency-base (MLE, smoothing) |  Neural network prediction  |\n",
        "| Sparsity Issue   | High for large n  |  No sparsity, embeddings help generalization  |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OsC0vGjliQm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "## Chapter 14.2 Intro to Neural Network LMs\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=125TcLRybNWL3WuIoqPU3-ZPmqMcw1Ro7)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8sWsJ7UPpzka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 14.3 Mathematical Notation for Neural Networks\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4bJ0wGu4rkwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single neuron representation of a perceptron:\n",
        "\n",
        "$$\n",
        " NN_{perceptron} (x) = xW + b\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   ``` x ``` is the input vector with ```d_in```\n",
        "*   ```W ``` is the weight matrix with ``` d_in x d_out ```\n",
        "*   ``` b ``` is the bias term with dimension ``` d_out```\n",
        "\n",
        "The output is calcu;ated by multiplying ther input vector by weight matrix and adding the bias.\n",
        "\n",
        "This is a linear model where:\n",
        "\n",
        "\n",
        "*   x.W represents matrix multiplication (or dot product for binary classification)\n",
        "*   The output is a weighted sum of inputs plus bias\n",
        "*   For binary classification, W is a vector and output is a single value\n",
        "*   For muti-class, W is a matrix with one column per class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eZz12J9qtp6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Example:\n",
        "Let's consider a perceptron for binary classification:\n",
        "\n",
        "Example: Email Spam Detection\n",
        "Inputs(x):\n",
        "\n",
        "\n",
        "*   x1 = 1 if email contains \"free\" (0 otherwise)\n",
        "*   x2 = 1 if email contains \"money\" (0 otherwise)\n",
        "*   x3 = 1 if email contains \"friend\" (0 otherwise)\n",
        "\n",
        "So x = [x1, x2, x3] with d_in = 3\n",
        "\n",
        "Weights (w):\n",
        "\n",
        "*   W = [0.5, 0.7,, -0.3] (where d_out = 1 for binary classification)\n",
        "\n",
        "Bias (b):\n",
        "\n",
        "\n",
        "*   b=-0.2\n",
        "\n",
        "For an email containing \"free\" and \"money\" but not \"friend\": x = [1,1,0] and output = x.W + b => (1*0.5) + (1*0.7) + 0*-0.3) + (-0.2) = 1.0\n",
        "\n",
        "If output > 0, classify as spam; otherwise, classify as not spam.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EkgrRTGkuuHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mathematical Notation with an Activation function:\n",
        "\n",
        "Video from Youtube:https://youtu.be/DaixewJTF8k?si=FW2rh3wGLsDjWGZQ\n",
        "Multilayer Perceptron:\n",
        "To add non-linearity, we introduce a hidden layer:\n",
        "\n"
      ],
      "metadata": {
        "id": "apwzugTSxPV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter # 17: Neural Networks I. (NLP Demystified)\n",
        "### *Course Source: NLP Demystified*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l52wZjWxnH-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 18: Neural Network II\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "3OoiPlNdnU7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bw8hVe7op1DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ATWGtdsbtlcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QOiaKd3VnSoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 19: Word Vectors\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "ZBCeIMoApLDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 20: Recurrent Neural Networks\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "V8tA3lYZp1tP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Otk0b0Ip7Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 21: Seq2Seq and Attention\n",
        "### *Course Source: NLP Demystified*"
      ],
      "metadata": {
        "id": "2w7Y3t-up-Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 22: Transformer\n",
        "`Course Source: NLP Demystified`"
      ],
      "metadata": {
        "id": "CCnci7w5qFz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SmBwsI-9qLIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 23: Parts of Speech & Grammars\n",
        "\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TsMKDzBqqPtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 24: Transformers and Neural Text Generation\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```\n"
      ],
      "metadata": {
        "id": "fC1xsNOLqbhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kLG6vt0Qqg6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 25: Encoder-Decoders, BERT and Fine-tuning\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "R7bxEQCGqiU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 26: Prompt Engineering, Instruction Following, and Using GPT\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "hCQ5lvUZqmxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 27: Machine Translation\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "dcVn20DzqsK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 28: Dialogue Systems, Chatbots & Question Answering\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "blvgKOWbq0oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 29: Parsing and Dependency Parsing\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "Tta28sZ6q6Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 30: Logical Representations of Sentence Meaning, Semantic Role Labelling & Information Extraction\n",
        "\n",
        "\n",
        "```\n",
        "UPENN CSI 5300\n",
        "```"
      ],
      "metadata": {
        "id": "ag7TM3frq-Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 31: Introduction to HuggingFace Transformers"
      ],
      "metadata": {
        "id": "PuUqFOZY6bNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QdDiDMCelwUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This library provides support for downloading, running, and training language models and is an essential tool for professionals that want to deploy NLP technology.\n",
        "\n",
        "\n",
        "1.   Transformers (Model inference): !pip install transformers\n",
        "2.   Accelerate (Model training): !pip install accelerate\n",
        "3.   Datasets (Data processing): !pip install datasets\n",
        "4.   Evaluate (Model evaluation): !pip install evaluate\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j9nO9nrt6wIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face 🤗 is a company and open-source community that's become the go-to place for working with state-of-the-art machine learning models, especially in natural language processing (NLP), though they've expanded into computer vision, audio, and more.\n",
        "\n",
        "When people say \"Hugging Face library,\" they're usually referring to:\n",
        "\n",
        "🤖 transformers library\n",
        "This is their most famous library.\n",
        "\n",
        "It gives you access to pre-trained models like:\n",
        "\n",
        "*   BERT\n",
        "*   GPT (including GPT-2, GPT-Neo, etc.)\n",
        "*   RoBERTa\n",
        "*   DistilBERT\n",
        "*   T5\n",
        "*   BLOOM\n",
        "*   Whisper (for speech-to-text)\n",
        "*   and many, many more"
      ],
      "metadata": {
        "id": "lQzF48Hs7WGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentimental Analysis using BERT"
      ],
      "metadata": {
        "id": "wXRh_fjM8FKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment-analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Use it!\n",
        "result = classifier(\"I love Hugging Face!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Ti0yL8Oo8Ds6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Masked Langauge Modeling\n"
      ],
      "metadata": {
        "id": "wbPgtaoJ9jM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download and use the BERT model for 'fill-mask'**"
      ],
      "metadata": {
        "id": "8wV245Nh9qn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install --upgrade accelerate\n",
        "!pip install evaluate\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from dill.source import getsource\n",
        "from collections import defaultdict\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device_id = 0 if str(device) == 'cuda' else -1\n",
        "\n",
        "bert_model = pipeline('fill-mask', model='bert-base-cased', device=device_id)"
      ],
      "metadata": {
        "id": "L2x52rdN9mpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_model(\"I went to an [MASK] restaurant and ordered pasta.\"))"
      ],
      "metadata": {
        "id": "fgDi74HyG4vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[{'sequence': 'I went to an Italian restaurant and ordered pasta.',\n",
        "  'score': 0.678,\n",
        "  'token': 12345,\n",
        "  'token_str': 'Italian'},\n",
        " {'sequence': 'I went to an amazing restaurant and ordered pasta.',\n",
        "  'score': 0.041,\n",
        "  'token': 6789,\n",
        "  'token_str': 'amazing'},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "BERT is telling you:\n",
        "\n",
        "\"Hmm, based on the sentence, 'Italian' is the most likely word to go in that blank.\"\n",
        "\n",
        "You're using a pre-trained BERT model to guess missing words in a sentence. It’s like intelligent fill-in-the-blank powered by deep learning — and it works in just a few lines of code!"
      ],
      "metadata": {
        "id": "Hb_9VwX9HAuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequence Classification with pre-trained BERT"
      ],
      "metadata": {
        "id": "bslL5CzuHYv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“`Sequence Classification with pre-trained BERT`” refers to using a pre-trained BERT model to classify entire sequences of text into categories — like spam vs. not spam, positive vs. negative sentiment, topic categories, etc.\n",
        "\n",
        "What is \"Sequence Classification\"?\n",
        "Sequence = a full text input (like a sentence, paragraph, or tweet)\n",
        "\n",
        "*   Sequence = a full text input (like a sentence, paragraph, or tweet)\n",
        "*   Classification = predicting a label or category for that input\n"
      ],
      "metadata": {
        "id": "rSV77wneHaId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model(\"I just saw a movie today and it was really great. My opinion of the movie is [MASK].\", targets=[\"positive\", \"negative\"])"
      ],
      "metadata": {
        "id": "eURyjpIjQinw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test BERT's ability to recognize sentiment using the Yelp Reviews dataset. This dataset consists of 650,000 reviews with their user-assigned star ratings. The task is to determine how many stars (1 to 5) the user gave given the text of their review."
      ],
      "metadata": {
        "id": "t1BW7wsJQnKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the yelp dataset from huggingface\n",
        "dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "id": "E9UCUS6vQm_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Default CacheLocation\n",
        "Hugging Face automatically downloads and caches the dataset locally — so it doesn't have to download it again next time.\n",
        "The dataset gets saved in:\n",
        "\n",
        "```~/.cache/huggingface/datasets/```\n",
        "\n",
        "More precisely:\n",
        "\n",
        "```~/.cache/huggingface/datasets/yelp_review_full/```\n",
        "\n",
        "### Want to check or change it?\n",
        "\n",
        "\n",
        "```\n",
        "from datasets import get_dataset_config_names, get_dataset_split_names\n",
        "from datasets.utils import hf_cache_home\n",
        "\n",
        "print(hf_cache_home)\n",
        "```\n",
        "\n",
        "Or override it by setting the environment variable:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "export HF_DATASETS_CACHE=\"/your/custom/path\"\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t92Y2XlIR8s5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##map()\n",
        "### 🧠 What is map() in Hugging Face datasets?\n",
        "It's a method that applies a function to every row in a dataset. Think of it like Python's map(), but optimized for working with big datasets and returning a new dataset object (like a copy with new columns or modified values).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def reverse_label(row):\n",
        "    return {'reverse_label': int(not row[\"binary_label\"])}\n",
        "\n",
        "```\n",
        "\n",
        "Or alternatively:\n",
        "\n",
        "\n",
        "```\n",
        "def reverse_label(row):\n",
        "    row['reverse_label'] = int(not row['binary_label'])\n",
        "    return row\n",
        "\n",
        "```\n",
        "\n",
        "Both of these take a row (which is a Python dictionary), compute the reverse of the binary_label (so 0 → 1 and 1 → 0), and return the updated row.\n",
        "\n",
        "Then you apply it to the dataset:\n",
        "\n",
        "\n",
        "```\n",
        "new_data = dataset.map(reverse_label)\n",
        "\n",
        "```\n",
        "\n",
        "Now `new_data` has a new column called `'reverse_label'`.\n"
      ],
      "metadata": {
        "id": "7TivehhiS1zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the yelp dataset from huggingface\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "# print 1\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# See label distribution\n",
        "from collections import Counter\n",
        "Counter([x['label'] for x in dataset['train']])"
      ],
      "metadata": {
        "id": "jPQy_rxVV-g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Mapping is non-destructive: It returns a new dataset — original is untouched.\n",
        "2.   You can use batched processing (faster for large datasets):\n"
      ],
      "metadata": {
        "id": "zLIvUGwJWNBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_label_batch(batch):\n",
        "    return {'reverse_label': [int(not x) for x in batch[\"binary_label\"]]}\n",
        "\n",
        "new_data = dataset.map(reverse_label_batch, batched=True)\n"
      ],
      "metadata": {
        "id": "sgLOPLJIWSiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rk9OJlaAih_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 Summary:\n",
        "1. map() is like a supercharged .apply() for Hugging Face datasets.\n",
        "\n",
        "2. It lets you transform or extend your dataset however you want.\n",
        "\n",
        "3. Ideal for preprocessing before model training.\n",
        "\n",
        "4. Let me know if you want to try it on a real dataset like Yelp reviews — we could e.g., turn star ratings into binary sentiment, or lowercase all the text!"
      ],
      "metadata": {
        "id": "ZFAo_tF-WbKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intro to the BERT Tokenizer**\n",
        "Before taking in a sequence of text, language models need to break up the sequence into tokens. Since BERT uses the [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model) architecture, it has a fixed maximum length of tokens it can process at any one time (that being 512). If you pass in a sequence of text longer than 512 tokens, BERT will throw an error:"
      ],
      "metadata": {
        "id": "vRm4Xlh7bC42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Giving BERT a piece of text with more than 512 tokens produces an error\n",
        "try:\n",
        "  bert_model(\"Hello \" * 512 + \" my name is [MASK]\")\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "4QZv8dCDbE2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ The 512 Token Limit: Why It Happens\n",
        "BERT (and most transformer-based models) was pretrained with a maximum input size of 512 tokens.\n",
        "\n",
        "So when you pass in a piece of text longer than 512 tokens, you get an error like:"
      ],
      "metadata": {
        "id": "rUpammfmbNUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Why the Limit Exists\n",
        "Transformers process text all at once (not word-by-word like RNNs).\n",
        "\n",
        "They compute attention between every token pair — so memory & computation scale quadratically with sequence length.\n",
        "\n",
        "To make training efficient, models like BERT limit input size to 512 tokens.\n",
        "\n",
        "📏 A \"token\" ≠ word! It's a subword piece (e.g., \"unbelievable\" → [\"un\", \"##believable\"])."
      ],
      "metadata": {
        "id": "OXyWjaQzbxgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ How to Fix It\n",
        "1. Truncate the input\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "tokenizer(text, max_length=512, truncation=True)\n",
        "This will keep only the first 512 tokens and discard the rest.\n",
        "\n",
        "2. Split long text into chunks\n",
        "If your input is long (e.g. an article or review), break it into overlapping 512-token windows and process them individually.\n",
        "\n",
        "3. Use models built for long inputs\n",
        "Try models like:\n",
        "\n",
        "Longformer (up to 4,096 tokens)\n",
        "\n",
        "BigBird (up to 4,096+ tokens)\n",
        "\n",
        "LED (Longformer Encoder-Decoder)\n",
        "\n",
        "GPT-NeoX, Claude, etc.\n",
        "\n",
        "These were designed to handle long documents efficiently."
      ],
      "metadata": {
        "id": "aUi4MfV9cVjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BERT Tokenizer\n",
        "\n",
        "🧠 What is a Tokenizer in BERT?\n",
        "A tokenizer is a tool that converts raw text into tokens — small units that the BERT model can understand.\n",
        "\n",
        "Think of it like the translator between human language and model language.\n",
        "\n",
        "🔧 What does it actually do?\n",
        "When you give it a sentence like:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\"I love deep learning!\"\n",
        "```\n",
        "The tokenizer:\n",
        "\n",
        "1. Splits the sentence into subword units\n",
        "\n",
        "2. Maps those subwords to numerical IDs\n",
        "\n",
        "3. Adds special tokens (like [CLS] and [SEP])\n",
        "\n",
        "4. Creates attention masks (so the model knows what to ignore)\n",
        "\n",
        "5. Pads/truncates the input to a fixed length\n"
      ],
      "metadata": {
        "id": "65DJTvDIcebj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filtering the data for BERT's max length** (5 points)\n",
        "Below we have provided code to load the BERT tokenizer. To run the tokenizer you can call it on a piece of text like `tokenizer(\"Hello students\")`. This will return a dictionary with three arrays, `input_ids`, `attention_mask` and `token_type_ids`. The length of all three arrays should be equal to the total number of tokens in the sequence.\n",
        "\n",
        "Since we need to make sure that no example in our input is longer than 512 tokens, your assignment now is to filter the dataset to only include examples that have 512 or less tokens. You can do this using the [Filter](https://huggingface.co/docs/datasets/process#select-and-filter) function. This function is similar to Map but returns a boolean where `True` indicates we should keep the row and `False` indicates we should remove it.\n",
        "\n",
        "NOTE: The maximum sequence length of any model can be found through the `tokenizer.model_max_length` variable. If you dislike magic numbers in your code, you can use this instead of hard-coding 512."
      ],
      "metadata": {
        "id": "G3IepZtzeFpU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAXUth1AcKoQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}